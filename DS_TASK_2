'''
SAMRUDDHI REDIJ
Task 2 LGMVIP DataScience
Stock Market Prediction And Forecasting Using Stacked LSTM

'''
from google.colab import files
uploaded = files.upload()
import pandas as pd
import numpy as np
import io
df = pd.read_csv(io.BytesIO(uploaded['stocks.csv']))
print(df)
Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.
Saving stocks.csv to stocks (2).csv
            Date    Open    High  ...   Close  Total Trade Quantity  Turnover (Lacs)
0     2018-09-28  234.05  235.95  ...  233.75               3069914          7162.35
1     2018-09-27  234.55  236.80  ...  233.25               5082859         11859.95
2     2018-09-26  240.00  240.00  ...  234.25               2240909          5248.60
3     2018-09-25  233.30  236.75  ...  236.10               2349368          5503.90
4     2018-09-24  233.55  239.20  ...  233.30               3423509          7999.55
...          ...     ...     ...  ...     ...                   ...              ...
2030  2010-07-27  117.60  119.50  ...  118.65                586100           694.98
2031  2010-07-26  120.10  121.00  ...  117.60                658440           780.01
2032  2010-07-23  121.80  121.95  ...  120.65                281312           340.31
2033  2010-07-22  120.30  122.00  ...  120.90                293312           355.17
2034  2010-07-21  122.10  123.00  ...  121.55                658666           803.56

[2035 rows x 8 columns]


import matplotlib. pyplot as plt
import matplotlib
%matplotlib inline
#importing libraries

from sklearn.preprocessing import MinMaxScaler
from keras.layers import LSTM, Dense, Dropout
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.dates as mandates
from sklearn.preprocessing import MinMaxScaler
from sklearn import linear_model
from keras.models import Sequential
from keras.layers import Dense
import keras.backend as K
from keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
from keras.models import load_model
from keras.layers import LSTM
from keras.utils.vis_utils import plot_model
print(df. shape)

print(df.isnull().values.any())
(2035, 8)
False
df.head
<bound method NDFrame.head of             Date    Open    High  ...   Close  Total Trade Quantity  Turnover (Lacs)
0     2018-09-28  234.05  235.95  ...  233.75               3069914          7162.35
1     2018-09-27  234.55  236.80  ...  233.25               5082859         11859.95
2     2018-09-26  240.00  240.00  ...  234.25               2240909          5248.60
3     2018-09-25  233.30  236.75  ...  236.10               2349368          5503.90
4     2018-09-24  233.55  239.20  ...  233.30               3423509          7999.55
...          ...     ...     ...  ...     ...                   ...              ...
2030  2010-07-27  117.60  119.50  ...  118.65                586100           694.98
2031  2010-07-26  120.10  121.00  ...  117.60                658440           780.01
2032  2010-07-23  121.80  121.95  ...  120.65                281312           340.31
2033  2010-07-22  120.30  122.00  ...  120.90                293312           355.17
2034  2010-07-21  122.10  123.00  ...  121.55                658666           803.56

[2035 rows x 8 columns]>
#Plotting the True Adjusted Close Value 
df['Close'].plot()
<matplotlib.axes._subplots.AxesSubplot at 0x7f70eb3efbd0>

#Setting the Target Variable and Selecting the Features
output_var = pd.DataFrame(df['Close'])
features = ['Open', 'High', 'Low', 'Turnover (Lacs)']
#Scaling
scaler = MinMaxScaler()
feature_transform = scaler.fit_transform(df[features])
feature_transform= pd.DataFrame(columns=features, data=feature_transform, index=df.index)
feature_transform.head()
Open	High	Low	Turnover (Lacs)
0	0.620235	0.622688	0.621560	0.127882
1	0.622263	0.626144	0.625285	0.212192
2	0.644363	0.639154	0.631078	0.093535
3	0.617194	0.625940	0.629009	0.098117
4	0.618208	0.635902	0.623836	0.142907
#Training and Testing Data
timesplit= TimeSeriesSplit(n_splits=10)
for train_index, test_index in timesplit.split(feature_transform):
        X_train, X_test = feature_transform[:len(train_index)], feature_transform[len(train_index): (len(train_index)+len(test_index))]
        y_train, y_test = output_var[:len(train_index)].values.ravel(), output_var[len(train_index): (len(train_index)+len(test_index))].values.ravel()
trainX =np.array(X_train)
testX =np.array(X_test)
X_train = trainX.reshape(X_train.shape[0], 1, X_train.shape[1])
X_test = testX.reshape(X_test.shape[0], 1, X_test.shape[1])
#processing the data for lstm
lstm = Sequential()
lstm.add(LSTM(25, input_shape=(1, trainX.shape[1]), activation='relu', return_sequences=False))
lstm.add(Dense(1))
lstm.compile(loss='mean_squared_error', optimizer='adam')
plot_model(lstm, show_shapes=True, show_layer_names=True)

#training the model
history = lstm.fit(X_train, y_train, epochs=100, batch_size=8, verbose=1, s
huffle=False)
Epoch 1/100
232/232 [==============================] - 2s 2ms/step - loss: 25814.8145
Epoch 2/100
232/232 [==============================] - 0s 2ms/step - loss: 25305.3066
Epoch 3/100
232/232 [==============================] - 0s 2ms/step - loss: 23829.9316
Epoch 4/100
232/232 [==============================] - 0s 2ms/step - loss: 21284.6094
Epoch 5/100
232/232 [==============================] - 0s 2ms/step - loss: 18071.1855
Epoch 6/100
232/232 [==============================] - 0s 2ms/step - loss: 14642.7988
Epoch 7/100
232/232 [==============================] - 0s 2ms/step - loss: 11326.6768
Epoch 8/100
232/232 [==============================] - 0s 2ms/step - loss: 8339.7754
Epoch 9/100
232/232 [==============================] - 0s 2ms/step - loss: 5814.7510
Epoch 10/100
232/232 [==============================] - 0s 2ms/step - loss: 3813.2915
Epoch 11/100
232/232 [==============================] - 0s 2ms/step - loss: 2328.8362
Epoch 12/100
232/232 [==============================] - 0s 2ms/step - loss: 1242.9675
Epoch 13/100
232/232 [==============================] - 0s 2ms/step - loss: 590.8105
Epoch 14/100
232/232 [==============================] - 0s 2ms/step - loss: 262.8464
Epoch 15/100
232/232 [==============================] - 0s 2ms/step - loss: 120.8805
Epoch 16/100
232/232 [==============================] - 0s 2ms/step - loss: 67.0380
Epoch 17/100
232/232 [==============================] - 0s 2ms/step - loss: 48.0105
Epoch 18/100
232/232 [==============================] - 0s 2ms/step - loss: 40.7240
Epoch 19/100
232/232 [==============================] - 0s 2ms/step - loss: 37.0301
Epoch 20/100
232/232 [==============================] - 0s 2ms/step - loss: 34.4468
Epoch 21/100
232/232 [==============================] - 0s 2ms/step - loss: 32.0740
Epoch 22/100
232/232 [==============================] - 0s 2ms/step - loss: 30.9805
Epoch 23/100
232/232 [==============================] - 0s 2ms/step - loss: 28.4258
Epoch 24/100
232/232 [==============================] - 0s 2ms/step - loss: 26.4791
Epoch 25/100
232/232 [==============================] - 0s 2ms/step - loss: 24.7430
Epoch 26/100
232/232 [==============================] - 0s 2ms/step - loss: 23.1718
Epoch 27/100
232/232 [==============================] - 0s 2ms/step - loss: 21.7599
Epoch 28/100
232/232 [==============================] - 0s 2ms/step - loss: 20.5017
Epoch 29/100
232/232 [==============================] - 0s 2ms/step - loss: 19.3851
Epoch 30/100
232/232 [==============================] - 0s 2ms/step - loss: 18.3925
Epoch 31/100
232/232 [==============================] - 0s 2ms/step - loss: 17.5028
Epoch 32/100
232/232 [==============================] - 0s 2ms/step - loss: 16.6948
Epoch 33/100
232/232 [==============================] - 0s 2ms/step - loss: 15.9500
Epoch 34/100
232/232 [==============================] - 0s 2ms/step - loss: 15.2532
Epoch 35/100
232/232 [==============================] - 0s 2ms/step - loss: 14.5939
Epoch 36/100
232/232 [==============================] - 0s 2ms/step - loss: 13.9647
Epoch 37/100
232/232 [==============================] - 0s 2ms/step - loss: 13.3610
Epoch 38/100
232/232 [==============================] - 0s 2ms/step - loss: 12.7801
Epoch 39/100
232/232 [==============================] - 0s 2ms/step - loss: 12.2200
Epoch 40/100
232/232 [==============================] - 0s 2ms/step - loss: 11.6797
Epoch 41/100
232/232 [==============================] - 0s 2ms/step - loss: 11.1584
Epoch 42/100
232/232 [==============================] - 0s 2ms/step - loss: 10.6558
Epoch 43/100
232/232 [==============================] - 0s 2ms/step - loss: 10.1717
Epoch 44/100
232/232 [==============================] - 0s 2ms/step - loss: 9.7063
Epoch 45/100
232/232 [==============================] - 0s 2ms/step - loss: 9.2599
Epoch 46/100
232/232 [==============================] - 0s 2ms/step - loss: 8.8329
Epoch 47/100
232/232 [==============================] - 0s 2ms/step - loss: 8.4259
Epoch 48/100
232/232 [==============================] - 0s 2ms/step - loss: 8.0392
Epoch 49/100
232/232 [==============================] - 0s 2ms/step - loss: 7.6734
Epoch 50/100
232/232 [==============================] - 0s 2ms/step - loss: 7.3287
Epoch 51/100
232/232 [==============================] - 0s 2ms/step - loss: 7.0057
Epoch 52/100
232/232 [==============================] - 0s 2ms/step - loss: 6.7043
Epoch 53/100
232/232 [==============================] - 0s 2ms/step - loss: 6.4247
Epoch 54/100
232/232 [==============================] - 0s 2ms/step - loss: 6.1668
Epoch 55/100
232/232 [==============================] - 0s 2ms/step - loss: 5.9304
Epoch 56/100
232/232 [==============================] - 0s 2ms/step - loss: 5.7151
Epoch 57/100
232/232 [==============================] - 0s 2ms/step - loss: 5.5204
Epoch 58/100
232/232 [==============================] - 0s 2ms/step - loss: 5.3458
Epoch 59/100
232/232 [==============================] - 0s 2ms/step - loss: 5.1903
Epoch 60/100
232/232 [==============================] - 1s 2ms/step - loss: 5.0532
Epoch 61/100
232/232 [==============================] - 0s 2ms/step - loss: 4.9333
Epoch 62/100
232/232 [==============================] - 0s 2ms/step - loss: 4.8295
Epoch 63/100
232/232 [==============================] - 0s 2ms/step - loss: 4.7405
Epoch 64/100
232/232 [==============================] - 0s 2ms/step - loss: 4.6648
Epoch 65/100
232/232 [==============================] - 0s 2ms/step - loss: 4.6012
Epoch 66/100
232/232 [==============================] - 0s 2ms/step - loss: 4.5480
Epoch 67/100
232/232 [==============================] - 0s 2ms/step - loss: 4.5038
Epoch 68/100
232/232 [==============================] - 0s 2ms/step - loss: 4.4673
Epoch 69/100
232/232 [==============================] - 0s 2ms/step - loss: 4.4372
Epoch 70/100
232/232 [==============================] - 0s 2ms/step - loss: 4.4122
Epoch 71/100
232/232 [==============================] - 0s 2ms/step - loss: 4.3913
Epoch 72/100
232/232 [==============================] - 0s 2ms/step - loss: 4.3737
Epoch 73/100
232/232 [==============================] - 0s 2ms/step - loss: 4.3586
Epoch 74/100
232/232 [==============================] - 0s 2ms/step - loss: 4.3455
Epoch 75/100
232/232 [==============================] - 0s 2ms/step - loss: 4.3339
Epoch 76/100
232/232 [==============================] - 0s 2ms/step - loss: 4.3234
Epoch 77/100
232/232 [==============================] - 0s 2ms/step - loss: 4.3139
Epoch 78/100
232/232 [==============================] - 0s 2ms/step - loss: 4.3050
Epoch 79/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2967
Epoch 80/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2889
Epoch 81/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2813
Epoch 82/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2741
Epoch 83/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2671
Epoch 84/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2602
Epoch 85/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2536
Epoch 86/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2470
Epoch 87/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2405
Epoch 88/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2342
Epoch 89/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2279
Epoch 90/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2216
Epoch 91/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2154
Epoch 92/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2093
Epoch 93/100
232/232 [==============================] - 0s 2ms/step - loss: 4.2032
Epoch 94/100
232/232 [==============================] - 0s 2ms/step - loss: 4.1971
Epoch 95/100
232/232 [==============================] - 0s 2ms/step - loss: 4.1911
Epoch 96/100
232/232 [==============================] - 0s 2ms/step - loss: 4.1851
Epoch 97/100
232/232 [==============================] - 0s 2ms/step - loss: 4.1791
Epoch 98/100
232/232 [==============================] - 0s 2ms/step - loss: 4.1731
Epoch 99/100
232/232 [==============================] - 0s 2ms/step - loss: 4.1672
Epoch 100/100
232/232 [==============================] - 0s 2ms/step - loss: 4.1613
#LSTM PREICTION
y_pred= lstm.predict(X_test)
y_pred
array([[101.83712 ],
       [102.7678  ],
       [107.08593 ],
       [107.44911 ],
       [105.78764 ],
       [105.219406],
       [101.23131 ],
       [ 98.70108 ],
       [ 97.179306],
       [ 97.05464 ],
       [ 97.40439 ],
       [ 97.91863 ],
       [ 99.09023 ],
       [ 98.41609 ],
       [ 97.65308 ],
       [ 98.15293 ],
       [ 98.40105 ],
       [ 98.022705],
       [ 97.90701 ],
       [ 99.46333 ],
       [ 99.22794 ],
       [102.49324 ],
       [ 99.8741  ],
       [ 95.90674 ],
       [ 95.68346 ],
       [ 90.655365],
       [ 89.25788 ],
       [ 90.51411 ],
       [ 90.82034 ],
       [ 90.15895 ],
       [ 87.84203 ],
       [ 88.4897  ],
       [ 92.399185],
       [ 95.91465 ],
       [ 96.18332 ],
       [ 95.08086 ],
       [ 98.58638 ],
       [ 97.34683 ],
       [ 94.897606],
       [ 94.71188 ],
       [ 92.9659  ],
       [ 89.3801  ],
       [ 87.1676  ],
       [ 90.59125 ],
       [ 94.46988 ],
       [ 96.46327 ],
       [ 95.85406 ],
       [ 96.08029 ],
       [ 97.99281 ],
       [ 99.798874],
       [ 97.88515 ],
       [103.12511 ],
       [106.15256 ],
       [106.25793 ],
       [104.77417 ],
       [105.645485],
       [105.663864],
       [104.83009 ],
       [105.03934 ],
       [105.08661 ],
       [105.48355 ],
       [103.12503 ],
       [102.09763 ],
       [103.28558 ],
       [106.339874],
       [110.079025],
       [111.45894 ],
       [112.11575 ],
       [112.11373 ],
       [109.24489 ],
       [107.67857 ],
       [108.89987 ],
       [109.48157 ],
       [109.34878 ],
       [111.41553 ],
       [112.21074 ],
       [112.83211 ],
       [114.75624 ],
       [115.88267 ],
       [114.24695 ],
       [113.69278 ],
       [114.162186],
       [113.05743 ],
       [112.325775],
       [109.93199 ],
       [113.26798 ],
       [114.08209 ],
       [114.26592 ],
       [113.467125],
       [114.10575 ],
       [115.26145 ],
       [113.53637 ],
       [112.01658 ],
       [111.79304 ],
       [109.15871 ],
       [113.99427 ],
       [116.966805],
       [116.02996 ],
       [118.61711 ],
       [119.53703 ],
       [119.37506 ],
       [122.82193 ],
       [124.12591 ],
       [126.40723 ],
       [130.48096 ],
       [128.9852  ],
       [125.34871 ],
       [125.08047 ],
       [127.05443 ],
       [123.76288 ],
       [124.75175 ],
       [124.63559 ],
       [125.120834],
       [124.754326],
       [128.0921  ],
       [126.980225],
       [127.666275],
       [128.71071 ],
       [129.87961 ],
       [129.16377 ],
       [128.72543 ],
       [129.98364 ],
       [128.34724 ],
       [131.74088 ],
       [133.39127 ],
       [131.78728 ],
       [132.5612  ],
       [133.78862 ],
       [133.21426 ],
       [133.13168 ],
       [127.702705],
       [124.72565 ],
       [123.58692 ],
       [121.51553 ],
       [120.66255 ],
       [122.35112 ],
       [123.34523 ],
       [124.70563 ],
       [124.15867 ],
       [121.35357 ],
       [121.339905],
       [122.982956],
       [122.49471 ],
       [122.729034],
       [122.76125 ],
       [124.06662 ],
       [125.249   ],
       [126.7494  ],
       [127.33413 ],
       [127.085655],
       [124.41574 ],
       [122.83215 ],
       [122.06615 ],
       [122.79486 ],
       [121.34079 ],
       [117.58229 ],
       [117.62971 ],
       [118.02429 ],
       [117.618515],
       [116.18613 ],
       [117.772446],
       [114.58291 ],
       [111.52533 ],
       [110.48692 ],
       [109.22631 ],
       [110.73134 ],
       [113.42976 ],
       [113.380936],
       [112.56979 ],
       [113.17799 ],
       [112.219475],
       [111.98726 ],
       [112.87139 ],
       [115.192635],
       [117.17204 ],
       [118.03047 ],
       [116.78849 ],
       [115.96239 ],
       [115.24442 ],
       [117.90763 ],
       [115.52094 ],
       [118.616806],
       [120.51958 ],
       [120.10506 ],
       [121.31049 ]], dtype=float32)
#True vs Predicted Adj Close Value â€“ LSTM
plt.plot(y_test, label='True Value')
plt.plot(y_pred, label='LSTM Value')
plt.title('Prediction by LSTM')
plt.xlabel('Time Scale')
plt.ylabel('Scaled Rupees')
plt.legend()
plt.show()

